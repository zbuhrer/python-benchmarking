# Python Performance Benchmarks

A collection of benchmarks exploring various aspects of Python performance. This project aims to provide empirical data about different Python constructs, helping developers make informed decisions about their code.

## Overview

This repository contains benchmarks for various Python operations, data structures, and programming patterns. Each benchmark is designed to provide clear, reproducible results that can guide practical development decisions.

## Database System

The project uses SQLite to store and track benchmark results. This enables:
- Systematic collection of benchmark data
- Historical performance tracking
- Cross-environment result comparison
- Data-driven performance analysis

### Database Setup

```sh
# Initialize the database and create required tables
python database.py
```

### Database Schema

The database consists of two main tables:

**benchmarks**
- `id`: Primary key
- `name`: Benchmark name
- `category`: Category (e.g., "data-structures", "string-operations")
- `description`: Detailed description of what is being tested
- `script_path`: Path to the benchmark script

**results**
- `id`: Primary key
- `benchmark_id`: Foreign key referencing benchmarks table
- `timestamp`: When the benchmark was run
- `execution_time`: Total execution time
- `output`: Detailed benchmark results

### Running Benchmarks

```sh
# Run all benchmarks and store results in database
python run_benchmarks.py

# Query and display results
python query_results.py
```

## Structure

```sh
python-benchmarking/
â”œâ”€â”€ benchmarks/                     # Main benchmarks directory
â”‚   â”œâ”€â”€ 1-data-structures/         # âœ“ Implemented
â”‚   â”‚   â”œâ”€â”€ dict-vs-list-membership.py
â”‚   â”‚   â”œâ”€â”€ list-vs-tuple-access.py
â”‚   â”‚   â””â”€â”€ set-vs-list-membership.py
â”‚   â”œâ”€â”€ 2-string-operations/       # ğŸ”„ In Progress
â”‚   â”œâ”€â”€ 3-function-calls/          # ğŸ“… Planned
â”‚   â”œâ”€â”€ 4-loops/                   # ğŸ“… Planned
â”‚   â”œâ”€â”€ 5-imports/                 # â³ Backlog
â”‚   â”œâ”€â”€ 6-exception-handling/      # â³ Backlog
â”‚   â”œâ”€â”€ 7-math-operations/         # â³ Backlog
â”‚   â”œâ”€â”€ 8-sorting/                 # â³ Backlog
â”‚   â”œâ”€â”€ 9-io-operations/           # â³ Backlog
â”‚   â”œâ”€â”€ 10-multithreading-async/   # â³ Backlog
â”‚   â”œâ”€â”€ 11-caching-lookup/         # â³ Backlog
â”‚   â”œâ”€â”€ 12-python-internals/       # â³ Backlog
â”‚   â””â”€â”€ results/                   # Benchmark results in markdown
â”‚       â”œâ”€â”€ 1-data-structures/
â”‚       â””â”€â”€ 2-string-operations/
â”œâ”€â”€ database.py                    # Database initialization and management
â”œâ”€â”€ main.py                        # Main application entry point
â”œâ”€â”€ query_results.py              # Tools for querying benchmark results
â””â”€â”€ run_benchmarks.py             # Benchmark execution script
```

## Results

Results are stored in the SQLite database and can be analyzed using the provided query tools. The system tracks:
- Execution times across different runs
- Performance variations between environments
- Historical trends

### Result Configurations

| Machine | CPU | RAM | Python Version | OS | Date | Notes |
|---------|-----|-----|----------------|----|----|-------|
|[zbuyhrer](https://github.com/zbuhrer) | Apple M2 | 16 GB  | Python 3.12 | MacOS Sonoma Version 14.5 (23F79) | Feb 2025  | Initial Benchmarking |

## Contributing

When adding new benchmarks:
1. Create a new benchmark script in the appropriate category directory
2. Ensure the script outputs results in the expected format
3. Run the benchmark using `run_benchmarks.py`
4. Verify results are properly stored in the database
5. Update documentation as needed

## Future Improvements

- Web interface for viewing and comparing results
- Automated environment detection and logging
- Statistical analysis of benchmark results
- Performance visualization tools
- Cross-version Python testing
